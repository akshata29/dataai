{
	"name": "4_spark deltalake",
	"properties": {
		"folder": {
			"name": "WideWorldImport/03_machine learning"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "dataaispk",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "59eeda99-9beb-431e-b1f3-29c9b566af2e"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/e2171f6d-2650-45e6-af7e-6d6e44ca92b1/resourceGroups/dataai/providers/Microsoft.Synapse/workspaces/dataaisynapsewks/bigDataPools/dataaispk",
				"name": "dataaispk",
				"type": "Spark",
				"endpoint": "https://dataaisynapsewks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/dataaispk",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.2",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Delta Lake features"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Use `spark.read.csv()` to load the data from the source public blob storage account and display its schema and shape."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.types import *\r\n",
					"import numpy as np\r\n",
					"import pandas as pd"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"manualSchema = StructType([\r\n",
					"  StructField(\"CustomerId\", StringType(), True),\r\n",
					"  StructField(\"ProductId\", StringType(), True),\r\n",
					"  StructField(\"Rating\", LongType(), True),\r\n",
					"  StructField(\"Cost\", FloatType(), True),\r\n",
					"  StructField(\"Size\", FloatType(), True),\r\n",
					"  StructField(\"Price\", FloatType(), True),\r\n",
					"  StructField(\"PrimaryBrandId\", LongType(), True),\r\n",
					"  StructField(\"GenderId\", LongType(), True),\r\n",
					"  StructField(\"MaritalStatus\", LongType(), True),\r\n",
					"  StructField(\"LowerIncomeBound\", FloatType(), True),\r\n",
					"  StructField(\"UpperIncomeBound\", FloatType(), True)\r\n",
					"])"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"url = \"abfss://wwi-02@dataairawdls.dfs.core.windows.net/data/PersonalizedData.csv\"\n",
					"raw_data = spark.read.csv(url, header=True, schema=manualSchema)\n",
					"print(\"Schema: \")\n",
					"raw_data.printSchema()\n",
					"\n",
					"df = raw_data.toPandas()\n",
					"print(\"Shape: \", df.shape)"
				],
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Save the customer rating dataframe as a Delta Lake table."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"delta_table_path = 'abfss://delta@dataairawdls.dfs.core.windows.net/customerrating'\n",
					"raw_data.write.format('delta').save(delta_table_path)\n",
					"mssparkutils.fs.ls(delta_table_path)"
				],
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Explore the layout of files and inspect the Delta log file."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"delta_log_path = mssparkutils.fs.ls(f'{delta_table_path}/_delta_log')[0].path\n",
					"print(delta_log_path)\n",
					"mssparkutils.fs.head(delta_log_path)"
				],
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Load the Delta lake table into a Spark dataframe."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"data = spark.read.format('delta').load(delta_table_path)\n",
					"data.show()"
				],
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Use the dedicated `DeltaTable` class to manage the Delta Lake table. Explore the Delta Lake table history."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# Get all versions\n",
					"from delta.tables import DeltaTable\n",
					"\n",
					"delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
					"display(delta_table.history())"
				],
				"execution_count": 10
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Perform an update on the Delta Lake table using a SQL-style condition."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Declare the predicate by using a SQL-formatted string.\n",
					"delta_table.update(\n",
					"  condition = \"Price < 1500\",\n",
					"  set = { \"Price\": \"Price * 1.05\" }\n",
					")\n",
					""
				],
				"execution_count": 11
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Check again the history of the Delta Lake table and notice the new entry corresponding to the update that has just been performed."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"display(delta_table.history())"
				],
				"execution_count": 12
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"It's possible to query previous snapshots of your Delta Lake table by using a feature called Time Travel. If you want to access the data that you overwrote, you can query a snapshot of the table before you overwrote the first set of data using the versionAsOf option."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"display(spark.read.format(\"delta\").option(\"versionAsOf\", \"0\").load(delta_table_path))"
				],
				"execution_count": 13
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"display(spark.read.format(\"delta\").option(\"versionAsOf\", \"1\").load(delta_table_path))\n",
					""
				],
				"execution_count": 14
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Create the metadata to expose the Delta Lake table in the default Spark database."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark.sql(\"CREATE TABLE CustomerRating USING DELTA LOCATION '{0}'\".format(delta_table_path))"
				],
				"execution_count": 15
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"List all tables that exist in the default Spark database."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark.sql(\"SHOW TABLES\").show()"
				],
				"execution_count": 16
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Explore the properties of the `CustomerRating` Spark database table."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark.sql(\"DESCRIBE EXTENDED customerrating\").show(truncate=False)"
				],
				"execution_count": 17
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"To query the delta table from the serverless SQL pool, navigate to the `Develop` hub in Synapse Studio and create a new SQL script. Make sure `Built-in` is selected for the `Connect to` option and `default` is selected for the `Use database` option.\n",
					"\n",
					"Enter the query below and make sure you replace `<your_data_lake_account_name>` with the name of the Data Lake account with the one from your lab environment.\n",
					"\n",
					"\n",
					"```sql\n",
					"SELECT TOP 10 *\n",
					"FROM OPENROWSET(\n",
					"    BULK 'abfss://delta@azrawdatalakeash.dfs.core.windows.net/customer-rating/',\n",
					"    FORMAT = 'delta') as rows\n",
					"```\n",
					"\n",
					"![Query Delta Lake with serverless SQL pool](https://solliancepublicdata.blob.core.windows.net/synapse-l400/notebook-images/query-delta-table.png)"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"This concludes the Delta Lake section of this notebook.\n",
					"\n",
					"To learn more about Delta Lake support in Syanspe Spark, take a look at the [Work with Delta Lake](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-delta-lake-overview?pivots=programming-language-python) section in the Azure Synapse Analytics documentation."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Mount storage account containers\n",
					"\n",
					"The `mssparkutils` utility can be used to mount storage account containers. In the example below, you will use an already created linked service to manage the authentication with the storage account."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#mssparkutils.fs.unmount(\"/test\", True)"
				],
				"execution_count": 19
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"mssparkutils.fs.mount( \n",
					"    \"abfss://delta@dataairawdls.dfs.core.windows.net\", \n",
					"    \"/test\", \n",
					"    {\"linkedService\":\"dataairawdls\"} \n",
					") "
				],
				"execution_count": 20
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Explore the content of the mounted volume using the local path. Note the `synfs:/{jobId}` prefix used by `mssparkutils` for the local path."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"jobId = mssparkutils.env.getJobId() \n",
					"\n",
					"log_files = mssparkutils.fs.ls(f'synfs:/{jobId}/test/customer-rating/_delta_log')\n",
					"log_files"
				],
				"execution_count": 21
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"When using regular PySpark classes, the syntax of the prefix is slightly different - `/synfs/{jobId}`. Use this prefix to load and display the first 500 characters from the first Delta Lake log file."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": false,
						"source_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"with open(f'/synfs/{jobId}/test/customer-rating/_delta_log/{log_files[0].name}', 'r') as f:\n",
					"    f.read()[:500]"
				],
				"execution_count": 22
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				]
			}
		]
	}
}