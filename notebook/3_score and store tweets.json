{
	"name": "3_score and store tweets",
	"properties": {
		"folder": {
			"name": "Risk"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "68dad71f-0ee7-49b7-ac56-3a370cc8067b"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"import pkg_resources\n",
					"for d in pkg_resources.working_set:\n",
					"     print(d)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"import urllib.parse, base64\n",
					"import json\n",
					"import requests\n",
					"import pandas as pd\n",
					"import datetime\n",
					"import pytz\n",
					"from azure.core.credentials import AzureKeyCredential\n",
					"from azure.ai.textanalytics import TextAnalyticsClient\n",
					"\n",
					"#sentiment_url = 'https://southcentralus.api.cognitive.microsoft.com/text/analytics/v3.0/sentiment' # service address \n",
					"sentiment_url = 'https://twittercog64.cognitiveservices.azure.com/'\n",
					"api_key = '2383634b398945b682b4c6ff9cd13cc5'          # Azure Cognitive API Key, replace with your own key\n",
					"\n",
					"\n",
					"credential = AzureKeyCredential(api_key)\n",
					"text_analytics_client = TextAnalyticsClient(endpoint=sentiment_url, credential=credential)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"from azure.storage.blob import BlobServiceClient, ContainerClient , BlobClient\n",
					"\n",
					"accountName = \"marketriskdl\"\n",
					"accountKey = \"2Gsb+co0ZjMiaiJxBHAPMhRyvY0NDz08r4WA2mDVgBd7RHcbykI+wUCYkIW9blVE/hnNXm+Z+mOzkppHi4BTag==\"\n",
					"containerName = \"risk\"\n",
					"connectionString = \"DefaultEndpointsProtocol=https;AccountName=marketriskdl;AccountKey=2Gsb+co0ZjMiaiJxBHAPMhRyvY0NDz08r4WA2mDVgBd7RHcbykI+wUCYkIW9blVE/hnNXm+Z+mOzkppHi4BTag==;EndpointSuffix=core.windows.net\"\n",
					"\n",
					"containerName = \"risk\"\n",
					"blobService = BlobServiceClient(connectionString)\n",
					"containerService = ContainerClient.from_connection_string(connectionString, containerName)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"import datetime\n",
					"import pytz\n",
					"\n",
					"currDate = datetime.datetime.now(pytz.timezone('US/Central'))\n",
					"blobName = \"TwitterData/\" + str(currDate.year) + \"/\" + str(currDate.month).zfill(2) + \"/\" + str(currDate.day -1) \n",
					"\n",
					"filename_prefix = \"/\" + blobName \n",
					"\n",
					"\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": true
				},
				"source": [
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# Read out CSV file with list of all tickers\r\n",
					"nqSdf = spark.read.load('abfss://risk@marketriskdl.dfs.core.windows.net/Portfolio/PortfolioCik.csv', \r\n",
					"    format='csv', \r\n",
					"    sep=\",\",\r\n",
					"    header=True)\r\n",
					"\r\n",
					"portfolioData = nqSdf.toPandas()\r\n",
					"display(portfolioData)\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"import glob\n",
					"import re \n",
					"\n",
					"#Get all the tweets generated for the symbols in the portfolio\n",
					"allticker = pd.DataFrame()\n",
					"for index, portfolio in portfolioData.iterrows():\n",
					"   print(\"ticker\", portfolio.Ticker)\n",
					"   if portfolio.Ticker :\n",
					"      print(\"prefix\", filename_prefix)\n",
					"      filename_objects = containerService.list_blobs(name_starts_with= filename_prefix)\n",
					"      filtered = [fileinfo.name for fileinfo in filename_objects if portfolio.Ticker in fileinfo.name]\n",
					"      print(filtered)\n",
					"\n",
					"      consolidatedtweets = pd.DataFrame()\n",
					"      scoredTweets = pd.DataFrame()\n",
					"\n",
					"\n",
					"#itterate over files and consolidate tweets \n",
					"      for tickerfilename in filtered : \n",
					"        absolouteFileName = \"abfss://risk@marketriskdl.dfs.core.windows.net/\" + tickerfilename\n",
					"        nqSdf = spark.read.format('csv') \\\n",
					"                    .option('header',True) \\\n",
					"                    .option('multiLine', True) \\\n",
					"                    .load(absolouteFileName)\n",
					"        print(\"fileName\" , absolouteFileName)\n",
					"        consolidatedtweets = consolidatedtweets.append(nqSdf.toPandas())\n",
					"     \n",
					"#Drop duplicates from consolidated         \n",
					"      print(\"Before drop\", consolidatedtweets.shape[0])\n",
					"      consolidatedtweets = consolidatedtweets.drop_duplicates(subset=['TweetText'])\n",
					"      print(\"After Drop\", consolidatedtweets.shape[0])\n",
					"      consolidatedtweets = consolidatedtweets[consolidatedtweets['TweetLanguage'] == 'en']  \n",
					"\n",
					"      consolidatedtweets['Location'] = consolidatedtweets['Location\\r'].str.strip(r'\\\\r').astype(str)\n",
					"      consolidatedtweets = consolidatedtweets.drop(columns='Location\\r')\n",
					"\n",
					"\n",
					"#Score and add to scoredtweets\n",
					"      for index, tweet in consolidatedtweets.iterrows():\n",
					"            documents = []\n",
					"            mentions = re.findall(\"@([a-zA-Z0-9]{1,15})\", tweet.TweetText)\n",
					"            topics = re.findall(\"#([a-zA-Z0-9]{1,15})\", tweet.TweetText)\n",
					"            documents.append (\"'\" + tweet.TweetText + \"'\")\n",
					"            for text in mentions: text = re.sub('\\W+',' ', text)\n",
					"            for text1 in topics: text1= re.sub('\\W+',' ', text1)\n",
					"            tweet['mentions'] =   mentions\n",
					"            tweet['topics'] =  topics\n",
					"            tweet['Ticker'] = portfolio.Ticker\n",
					"            response = text_analytics_client.analyze_sentiment(documents, language=\"en\")\n",
					"            result = [doc for doc in response if not doc.is_error]\n",
					"            tweet['sentiment'] = result[0].sentiment\n",
					"            tweet['positive'] = result[0].confidence_scores.positive\n",
					"            tweet['negative'] = result[0].confidence_scores.negative\n",
					"            tweet['neutral'] = result[0].confidence_scores.neutral\n",
					"            tweet.TweetText = re.sub('\\W+',' ', tweet.TweetText)\n",
					"\n",
					"            if tweet.Location : \n",
					"              tweet.Location = tweet.Location.rstrip('\\r')\n",
					"            else :\n",
					"              tweet.Location = \" \"\n",
					"              tweet.Location = tweet.Location.rstrip('\\r')\n",
					"            \n",
					"            scoredTweets = scoredTweets.append(tweet,ignore_index=True) \n",
					"      allticker = allticker.append(scoredTweets)\n",
					"      output = scoredTweets.to_csv(index=False, header=True, encoding = \"utf-8\")\n",
					"      \n",
					"\n",
					"      blobName = \"TwitterData/scored/\" + str(currDate.year) + \"/\" + str(currDate.month) + \"/\" + str(currDate.day) + \"/\" + portfolio.Ticker + \".csv\"\n",
					"      blob = BlobClient.from_connection_string(conn_str=connectionString, container_name=containerName, blob_name=blobName)\n",
					"      blob.upload_blob(output)\n",
					"    \n",
					"\n",
					" outputall = allticker.to_csv(index=False, header=True, encoding = \"utf-8\")\n",
					"      \n",
					"\n",
					"  blobNameall = \"TwitterData/scored/\" + str(currDate.year) + \"/\" + str(currDate.month) + \"/\" + str(currDate.day) + \"/\" + \"allticker\" + \".csv\"\n",
					"  bloball = BlobClient.from_connection_string(conn_str=connectionString, container_name=containerName, blob_name=blobNameall)\n",
					"  bloball.upload_blob(outputall)\n",
					"\n",
					"      \n",
					"    \n",
					"        \n",
					"\n",
					"\n",
					"\n",
					"\n",
					"\n",
					"\n",
					"           \n",
					"\n",
					"\n",
					"  \n",
					"\n",
					""
				],
				"execution_count": 14
			}
		]
	}
}