{
	"name": "3_score and store tweets_0",
	"properties": {
		"folder": {
			"name": "Risk"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "b4123faf-aa65-4abd-8580-fe9e6c649918"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"import pip #needed to use the pip functions\n",
					"for i in pip.get_installed_distributions(local_only=True):print(i)"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"source": [
					"import urllib.parse, base64\n",
					"import json\n",
					"import requests\n",
					"import pandas as pd\n",
					"import datetime\n",
					"import pytz\n",
					"from azure.core.credentials import AzureKeyCredential\n",
					"from azure.ai.textanalytics import TextAnalyticsClient\n",
					"\n",
					"#sentiment_url = 'https://southcentralus.api.cognitive.microsoft.com/text/analytics/v3.0/sentiment' # service address \n",
					"sentiment_url = 'https://markettweetsentimentanalysis.cognitiveservices.azure.com/'\n",
					"api_key = '4d3f9732c7f947cebc33c14db5c94b4e'          # Azure Cognitive API Key, replace with your own key\n",
					"\n",
					"\n",
					"credential = AzureKeyCredential(api_key)\n",
					"text_analytics_client = TextAnalyticsClient(endpoint=sentiment_url, credential=credential)"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"source": [
					"from azure.storage.blob import (\n",
					"    BlockBlobService\n",
					")\n",
					"\n",
					"accountName = \"marketriskdl\"\n",
					"accountKey = \"2Gsb+co0ZjMiaiJxBHAPMhRyvY0NDz08r4WA2mDVgBd7RHcbykI+wUCYkIW9blVE/hnNXm+Z+mOzkppHi4BTag==\"\n",
					"containerName = \"risk\"\n",
					"\n",
					"blobService = BlockBlobService(account_name=accountName, account_key=accountKey)"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"source": [
					"import datetime\n",
					"import pytz\n",
					"\n",
					"currDate = datetime.datetime.now(pytz.timezone('US/Central'))\n",
					"blobName = \"TwitterData/\" + str(currDate.year) + \"/\" + str(currDate.month) + \"/\" + str(currDate.day - 1)\n",
					"print(blobName)\n",
					"\n",
					"   "
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# Read out CSV file with list of all tickers\r\n",
					"nqSdf = spark.read.load('abfss://risk@marketriskdl.dfs.core.windows.net/Portfolio/PortfolioCik.csv', \r\n",
					"    format='csv', \r\n",
					"    sep=\",\",\r\n",
					"    header=True)\r\n",
					"\r\n",
					"portfolioData = nqSdf.toPandas()\r\n",
					"display(portfolioData)"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"source": [
					"import glob\n",
					"import re \n",
					"\n",
					"filename_prefix = \"/\" + blobName \n",
					"filename_objects = blobService.list_blobs('risk', prefix = filename_prefix)\n",
					"#print(filename_objects)\n",
					"\n",
					"for index, portfolio in portfolioData.iterrows():\n",
					"   print(portfolio.Ticker)\n",
					"   if portfolio.Ticker :\n",
					"      filtered = [fileinfo.name for fileinfo in filename_objects if portfolio.Ticker in fileinfo.name]\n",
					"      print(filtered)\n",
					"      consolidatedtweets = pd.DataFrame()\n",
					"\n",
					"      for tickerfilename in filtered : \n",
					"        absolouteFileName = \"abfss://risk@marketriskdl.dfs.core.windows.net/\" + tickerfilename\n",
					"        print(absolouteFileName)\n",
					"        nqSdf = spark.read.format('csv') \\\n",
					"                    .option('header',True) \\\n",
					"                    .option('multiLine', True) \\\n",
					"                    .load(absolouteFileName)\n",
					"        \n",
					"        consolidatedtweets = consolidatedtweets.append(nqSdf.toPandas())\n",
					"        scoredTweets = pd.DataFrame()\n",
					"        consolidatedtweets['Location'] = consolidatedtweets['Location\\r'].str.strip(r'\\\\r').astype(str)\n",
					"        consolidatedtweets = consolidatedtweets.drop(columns='Location\\r')\n",
					"        \n",
					"      print(\"Before drop\", consolidatedtweets.shape[0])\n",
					"      consolidatedtweets = consolidatedtweets.drop_duplicates(subset=['TweetText'])\n",
					"      print(\"After Drop\", consolidatedtweets.shape[0])\n",
					"\n",
					"      for index, tweet in consolidatedtweets.iterrows():\n",
					"            documents = []\n",
					"            mentions = re.findall(\"@([a-zA-Z0-9]{1,15})\", tweet.TweetText)\n",
					"            topics = re.findall(\"#([a-zA-Z0-9]{1,15})\", tweet.TweetText)\n",
					"            documents.append (\"'\" + tweet.TweetText + \"'\")\n",
					"            tweet['mentions'] = mentions\n",
					"            tweet['topics'] = topics\n",
					"            tweet['Ticker'] = portfolio.Ticker\n",
					"            response = text_analytics_client.analyze_sentiment(documents, language=\"en\")\n",
					"            result = [doc for doc in response if not doc.is_error]\n",
					"            tweet['sentiment'] = result[0].sentiment\n",
					"            tweet['positive'] = result[0].confidence_scores.positive\n",
					"            tweet['negative'] = result[0].confidence_scores.negative\n",
					"            tweet['neutral'] = result[0].confidence_scores.neutral\n",
					"\n",
					"            if tweet.Location : \n",
					"              tweet.Location = tweet.Location.rstrip('\\r')\n",
					"            else :\n",
					"              tweet.Location = \" \"\n",
					"              tweet.Location = tweet.Location.rstrip('\\r')\n",
					"            \n",
					"            scoredTweets = scoredTweets.append(tweet,ignore_index=True) \n",
					"\n",
					"      output = scoredTweets.to_csv(index=False, header=True, encoding = \"utf-8\")\n",
					"      blobService.create_blob_from_text(containerName,  \"TwitterData/scored/\" + str(currDate.year) + \"/\" + str(currDate.month) + \"/\" + str(currDate.day) + \"/\" + portfolio.Ticker + \".csv\", output)\n",
					"    \n",
					"\n",
					"\n",
					"\n",
					"      \n",
					"    \n",
					"        \n",
					"\n",
					"\n",
					"\n",
					"\n",
					"\n",
					"\n",
					"           \n",
					"\n",
					"\n",
					"  \n",
					"\n",
					""
				],
				"execution_count": 16
			}
		]
	}
}