{
	"name": "1_news and sentiment",
	"properties": {
		"folder": {
			"name": "Risk"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "dataaispk",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "b2f5428e-3c57-47b7-a51f-b5a418ba6646"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/e2171f6d-2650-45e6-af7e-6d6e44ca92b1/resourceGroups/dataai/providers/Microsoft.Synapse/workspaces/dataaisynapsewks/bigDataPools/dataaispk",
				"name": "dataaispk",
				"type": "Spark",
				"endpoint": "https://dataaisynapsewks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/dataaispk",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.2",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"diagram": {
						"activateDiagramType": 1,
						"chartConfig": {
							"category": "bar",
							"keys": [
								"Portfolio"
							],
							"values": [
								"Portfolio"
							],
							"yLabel": "Portfolio",
							"xLabel": "Portfolio",
							"aggregation": "COUNT",
							"aggByBackend": false
						},
						"aggData": "{\"Portfolio\":{\"Covid\":15,\"LongTerm\":9}}",
						"isSummary": false,
						"previewData": {
							"filter": null
						},
						"isSql": false
					},
					"collapsed": false
				},
				"source": [
					"# Read out CSV file with list of all tickers\n",
					"nqSdf = spark.read.load('abfss://risk@marketriskdl.dfs.core.windows.net/Portfolio/PortfolioCik.csv', \n",
					"    format='csv', \n",
					"    sep=\",\",\n",
					"    header=True)\n",
					"\n",
					"portfolioData = nqSdf.toPandas()\n",
					"display(portfolioData)"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"source": [
					"from azure.storage.blob import BlobServiceClient, ContainerClient\n",
					"from azure.storage.blob import BlobClient\n",
					"\n",
					"accountName = \"marketriskdl\"\n",
					"accountKey = \"2Gsb+co0ZjMiaiJxBHAPMhRyvY0NDz08r4WA2mDVgBd7RHcbykI+wUCYkIW9blVE/hnNXm+Z+mOzkppHi4BTag==\"\n",
					"containerName = \"risk\"\n",
					"connectionString = \"DefaultEndpointsProtocol=https;AccountName=marketriskdl;AccountKey=2Gsb+co0ZjMiaiJxBHAPMhRyvY0NDz08r4WA2mDVgBd7RHcbykI+wUCYkIW9blVE/hnNXm+Z+mOzkppHi4BTag==;EndpointSuffix=core.windows.net\""
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"source": [
					"import urllib.parse, base64\n",
					"import json\n",
					"import requests\n",
					"import pandas as pd\n",
					"import datetime\n",
					"import pytz\n",
					"from azure.core.credentials import AzureKeyCredential\n",
					"from azure.ai.textanalytics import TextAnalyticsClient\n",
					"\n",
					"search_url = \"https://southcentralus.api.cognitive.microsoft.com/bing/v7.0/news/search\"\n",
					"#sentiment_url = 'https://southcentralus.api.cognitive.microsoft.com/text/analytics/v3.0/sentiment' # service address \n",
					"sentiment_url = 'https://southcentralus.api.cognitive.microsoft.com'\n",
					"api_key = 'c62e1aa4119b4803a86a93f3a6062d42'          # Azure Cognitive API Key, replace with your own key\n",
					"\n",
					"\n",
					"credential = AzureKeyCredential(api_key)\n",
					"text_analytics_client = TextAnalyticsClient(endpoint=sentiment_url, credential=credential)"
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"source": [
					"#for row in nqDf.rdd.collect():\n",
					"for index, ticker in portfolioData.iterrows():\n",
					"\n",
					"    dfObj = pd.DataFrame(columns=['Symbol', 'Name', 'Url', 'Date_Published', 'Description', 'Sentiment', 'Positive_Score', 'Negative_Score', 'Neutral_Score'])\n",
					"\n",
					"    headers = {'Ocp-Apim-Subscription-Key':api_key, \\\n",
					"                'Content-type': 'application/json',\\\n",
					"                'Accept': 'application/json'}\n",
					"\n",
					"    symbol = ticker['Ticker']\n",
					"\n",
					"    params = urllib.parse.urlencode({\n",
					"        # Request parameters\n",
					"        'q': ticker['Ticker'],\n",
					"        'offset': '0',\n",
					"        'mkt': 'en-us',\n",
					"        'freshness': 'Day',\n",
					"        'count' : 100\n",
					"    })\n",
					"\n",
					"    response = requests.get(search_url, headers=headers, params=params)\n",
					"    response.raise_for_status()\n",
					"    search_results = response.json()\n",
					"    #print(json.dumps(search_results['value'], indent=2, sort_keys=True))\n",
					"    for value in search_results['value']:\n",
					"        description = value['description']\n",
					"        url = value['url']\n",
					"        name = value['name']\n",
					"        publishedDate = value['datePublished']\n",
					"    \n",
					"        documents = [\"'\" + description + \"'\"]\n",
					"\n",
					"        response = text_analytics_client.analyze_sentiment(documents, language=\"en\")\n",
					"        result = [doc for doc in response if not doc.is_error]\n",
					"        #print(result)\n",
					"\n",
					"        for doc in result:\n",
					"            #print(\"Overall sentiment: {}\".format(doc.sentiment))\n",
					"            #print(\"Scores: positive={}; neutral={}; negative={} \\n\".format(\n",
					"            #    doc.confidence_scores.positive,\n",
					"            #    doc.confidence_scores.neutral,\n",
					"            #    doc.confidence_scores.negative,\n",
					"            #))\n",
					"            dfObj = dfObj.append({'Symbol': symbol, 'Name': name, 'Url': url, 'Date_Published': publishedDate, 'Description' : description, 'Sentiment': doc.sentiment, 'Positive_Score':doc.confidence_scores.positive, 'Negative_Score': doc.confidence_scores.negative, 'Neutral_Score': doc.confidence_scores.neutral}, ignore_index=True)\n",
					"    \n",
					"    currDate = datetime.datetime.now(pytz.timezone('US/Central'))\n",
					"    output = dfObj.to_csv (index=False, header=True, encoding = \"utf-8\")\n",
					"    blobName = \"News/DailyData/\" + str(currDate.year) + \"/\" + str(currDate.month) + \"/\" + str(currDate.day) + \"/\" + ticker['Ticker'] + \".csv\"\n",
					"    blob = BlobClient.from_connection_string(conn_str=connectionString, container_name=containerName, blob_name=blobName)\n",
					"    blob.upload_blob(output)"
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}