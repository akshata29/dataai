{
	"name": "0_model training",
	"properties": {
		"folder": {
			"name": "WideWorldImporter/03_machine learning"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "dataaispk",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "6fe63cb9-392c-456c-9bd2-33eb178e1d94"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/e2171f6d-2650-45e6-af7e-6d6e44ca92b1/resourceGroups/dataai/providers/Microsoft.Synapse/workspaces/dataaisynapsewks/bigDataPools/dataaispk",
				"name": "dataaispk",
				"type": "Spark",
				"endpoint": "https://dataaisynapsewks.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/dataaispk",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.2",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"# Model training and registration\n",
					"This notebook show the process for training the model, converting the model to ONNX and uploading the ONNX model to Azure Storage."
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Explore the training data\n",
					"The following cells load the source CSV file into a Spark DataFrame and create a temporary view that can be used to query the data with Spark SQL.\n",
					"\n",
					"WWI has provided a small CSV file you can use for showing the process of training a simple model.\n",
					"\n",
					"They have already loaded for you in the data lake. \n",
					"It is located under the `wwi-02` container with the path `/sale-csv/wwi-factsale.csv`.\n",
					"You need to build the correct path to the file and the run the cells that follow to load and query the data.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"df = spark.read.load('abfss://wwi-02@dataairawdls.dfs.core.windows.net/sale-csv/wwi-factsale.csv', format=\"csv\"\n",
					", header=True, sep=\"|\"\n",
					")"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"source": [
					"Next, WWI would like you to show them how create a temporary view over the loaded dataframe.\n",
					"\n",
					"The view should be named `facts`.\n",
					"\n",
					"Complete the code in the cell and run it.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"df.createOrReplaceTempView(\"facts\")"
				],
				"execution_count": 2
			},
			{
				"cell_type": "markdown",
				"source": [
					"In the next cell, WWI would like you to explore the data with an initial query.\n",
					"\n",
					"You want to preview all of the sales having the `Customer Key` of `11`.\n",
					"\n",
					"You should order the results by `Stock Item Key`.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"spark.sql(\"SELECT * FROM facts WHERE `Customer Key` == '11' ORDER BY `Stock Item Key`\").show()"
				],
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Predict Quantity given Customer Key and Stock Item Key\n",
					"In the following cells we load a subset of the data that just contains the fields needed for training. \n",
					"\n",
					"WWI's data scientists have already provided some of the code for you. \n",
					"\n",
					"Read thru and run the following cells.\n",
					"\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import col\n",
					"df3 = spark.sql(\"SELECT double(`Customer Key`) as customerkey, double(`Stock Item Key`) as stockitemkey, double(`Quantity`) as quantity FROM facts\").where(col(\"quantity\").isNotNull())\n",
					"df3.cache()"
				],
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"source": [
					"Next, we package the data into the format expected by Spark ML's LinearRegression. It requires a DataFrame with two columns- `features` and a column with the labels to predict (`quantity` in this case).\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.ml.feature import VectorAssembler\n",
					"\n",
					"vectorAssembler = VectorAssembler(inputCols = ['customerkey', 'stockitemkey'], outputCol = 'features')\n",
					"df4 = vectorAssembler.transform(df3)\n",
					"df5 = df4.select(['features', 'quantity'])\n",
					"df5.show(10)"
				],
				"execution_count": 5
			},
			{
				"cell_type": "markdown",
				"source": [
					"Now, we split our DataFrame into training and testing DataFrames.\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"A best practice is to split data into training and test sets.\n",
					"\n",
					"WWI would like you to complete the final line that produces the train and test dataframes. \n",
					"\n",
					"Once you have completed the cell, run it.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"trainingFraction = 0.7\n",
					"testingFraction = (1-trainingFraction)\n",
					"seed = 42\n",
					"\n",
					"# Split the dataframe into test and training dataframes\n",
					"df_train, df_test = df5.randomSplit([trainingFraction, testingFraction], seed=seed)"
				],
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"source": [
					"In the following cell, you will train your LinearRegression model.\n",
					"\n",
					"The goal of this regressor is to predict the `quantity` field given all of the features. \n",
					"\n",
					"Complete the missing parameters and the last line to train the model.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.ml.regression import LinearRegression\n",
					"\n",
					"lin_reg = LinearRegression(featuresCol = 'features', labelCol='quantity', maxIter = 10, regParam=0.3)\n",
					"lin_reg_model = lin_reg.fit(df_train)\n",
					"\n",
					"print(\"Coefficients: \" + str(lin_reg_model.coefficients))\n",
					"print(\"Intercept: \" + str(lin_reg_model.intercept))"
				],
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"source": [
					"Now that you have a trained model in hand, WWI wants to verify you can use it to make predictions against the test DataFrame.\n",
					"\n",
					"Complete the first line to use your trained model to make predictions against the `df_test` dataframe.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"collapsed": false
				},
				"source": [
					"df_pred = lin_reg_model.transform(df_test)\n",
					"display(df_pred)"
				],
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Convert model to ONNX\n",
					"In the cells that follow, WWI wants you to show how you convert the model to ONNX and show how an output of how ONNX represents the Spark ML model.\n",
					"\n",
					"They have already provided you the code, you just need to run the cells.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from onnxmltools import convert_sparkml\n",
					"from onnxmltools.convert.common.data_types import FloatTensorType\n",
					"\n",
					"initial_types = [ \n",
					"    (\"features\", FloatTensorType([1, lin_reg_model.numFeatures])),\n",
					"    # (repeat for the required inputs)\n",
					"]"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"source": [
					"model_onnx = convert_sparkml(lin_reg_model, 'sparkml GeneralizedLinearRegression', initial_types)\n",
					"model_onnx"
				],
				"execution_count": 10
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Upload the model to Azure Storage\n",
					"\n",
					"In order for an ONNX model to be used by the T-SQL predict statement, it must be uploaded to Azure Storage.\n",
					"\n",
					"WWI wants you to show them how they would serialize the model to disk and then upload the model file to Azure Storage.\n",
					"\n",
					"Run the following cell to save  the ONNX model to the storage of the Spark driver node temporarily. "
				]
			},
			{
				"cell_type": "code",
				"source": [
					"with open(\"model.onnx\", \"wb\") as f:\n",
					"    f.write(model_onnx.SerializeToString())"
				],
				"execution_count": 11
			},
			{
				"cell_type": "markdown",
				"source": [
					"Next, you need to show WWI how to use the Azure Storage Python SDK to upload the ONNX model to Azure Storage.\n",
					"\n",
					"Complete the connection string with the correct values for your non-hierarchical Storage Account.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"modelPath = 'abfss://wwi-02@dataairawdls.dfs.core.windows.net/ml/onnx/product_seasonality_classifier.onnx'\n",
					"modelString = str(model_onnx.SerializeToString())\n",
					"mssparkutils.fs.put(modelPath, modelString)\n",
					""
				],
				"execution_count": 13
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				]
			}
		]
	}
}