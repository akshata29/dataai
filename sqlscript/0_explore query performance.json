{
	"name": "0_explore query performance",
	"properties": {
		"folder": {
			"name": "WideWorldImport/03_datawarehouse optimization"
		},
		"content": {
			"query": "\n-- The script takes around 24 seconds to execute and returns a count of ~ 340 million rows in the table.\nSELECT  \n    COUNT_BIG(*)\nFROM\n    [wwi_staging].[SaleHeap]\n\n-- More Complex Statement\n-- The script takes up to a 70 seconds to execute and returns the result. \n-- There is clearly something wrong with the Sale_Heap table that induces the performance hit.\n-- Note the OPTION clause used in the statement. This comes in handy when you're looking to \n-- identify your query in the sys.dm_pdw_exec_requests DMV.\n-- The ROUND_ROBIN distribution and The HEAP structure of the table could be the reason for performance hit\nSELECT TOP 1000 * FROM\n(\n    SELECT\n        S.CustomerId\n        ,SUM(S.TotalAmount) as TotalAmount\n    FROM\n        [wwi_staging].[SaleHeap] S\n    GROUP BY\n        S.CustomerId\n) T\nOPTION (LABEL = 'Sales Heap')\n\n-- Run this time with the EXPLAIN WITH_RECOMMENDATIONS line before it\n-- The EXPLAIN WITH_RECOMMENDATIONS clause returns the query plan for an Azure Synapse Analytics SQL statement without running the statement.\n--  Use EXPLAIN to preview which operations will require data movement and to view the estimated costs of the query operations. \n-- By default, you will get the execution plan in XML format, which you can export to other formats like CSV or JSON.\n--  Do not select Query Plan from the toolbar as it will try do download the query plan and open it in SQL Server Management Studio.\nEXPLAIN WITH_RECOMMENDATIONS\nSELECT TOP 1000 * FROM\n(\n    SELECT\n        S.CustomerId\n        ,SUM(S.TotalAmount) as TotalAmount\n    FROM\n        [wwi_staging].[SaleHeap] S\n    GROUP BY\n        S.CustomerId\n) T\n\n-- Notice the details of the internal layout of the MPP system:\n-- This layout is given by the current Date Warehouse Units (DWU) setting. In the setup used for the example above,\n--  we were running at DW100c which means that there is a single physical node to service the 60 distributions, giving a number of, again, \n-- 60 distributions per physical node. Depending on your own DWU settings, these numbers will vary.\n-- The query plan indicates data movement is required. This is indicated by the SHUFFLE_MOVE distributed SQL operation. \n-- Data movement is an operation where parts of the distributed tables are moved to different nodes during query execution. \n-- This operation is required where the data is not available on the target node, most commonly when the tables do not share the distribution key. \n-- The most common data movement operation is shuffle. During shuffle, for each input row, Synapse computes a hash value using the join \n-- columns and then sends that row to the node that owns that hash value. Either one or both sides of join can participate in the shuffle. \n\n-- Besides the EXPLAIN statement, you can also understand the plan details using the sys.dm_pdw_request_steps DMV.\nSELECT  \n    *\nFROM    \n    sys.dm_pdw_exec_requests\nWHERE   \n    [label] = 'Sales Heap'\n\n-- Grab QID from above query\nSELECT\n   *\nFROM\n    sys.dm_pdw_request_steps\nWHERE\n    request_id = 'QID3984'\nORDER BY\n   step_index\n\n-- The steps (indexed 0 to 4) are matching operations 2 to 6 from the query plan. Again, the culprit stands out: the step with index 2 \n-- describes the inter-partition data movement operation. By looking at the TOTAL_ELAPSED_TIME column one can clearly tell the \n-- largest part of the query time is generated by this step.\n\n-- Get more details on the problematic step using the following SQL statement \n-- (replace the request_id and step_index values from the previous query results):\nSELECT\n*\nFROM\n    sys.dm_pdw_sql_requests\nWHERE\n    request_id = 'QID3984'\n    AND step_index = 2\nORDER BY\n    distribution_id\n\n-- The results of the statement provide details about data being moved at each distribution. The ROWS_PROCESSED column is especially \n-- useful here to get an estimate of the magnitude of the data movement happening when the query is executed.\n\n--  Improve table structure with hash distribution and columnstore index\n-- CTAS is a more customizable version of the SELECT...INTO statement. SELECT...INTO doesn't allow you to change either the \n-- distribution method or the index type as part of the operation. You create the new table by using the default distribution \n-- type of ROUND_ROBIN, and the default table structure of CLUSTERED COLUMNSTORE INDEX.\nCREATE TABLE [wwi_perf].[SaleHash]\nWITH\n(\n    DISTRIBUTION = HASH ( [CustomerId] ),\n    CLUSTERED COLUMNSTORE INDEX\n)\nAS\nSELECT\n    *\nFROM\n    [wwi_staging].[SaleHeap] \n\n-- Run the query again to see the performance improvements:\nSELECT TOP 1000 * FROM\n(\n    SELECT\n        S.CustomerId\n        ,SUM(S.TotalAmount) as TotalAmount\n    FROM\n        [wwi_perf].[SaleHash]  S\n    GROUP BY\n        S.CustomerId\n) T\n\n-- Run the following EXPLAIN statement again to get the query plan \n-- (do not select Query Plan from the toolbar as it will try do download the query plan and open it in SQL Server Management Studio):\nEXPLAIN\nSELECT TOP 1000 * FROM\n(\n    SELECT\n        S.CustomerId\n        ,SUM(S.TotalAmount) as TotalAmount\n    FROM\n        [wwi_perf].[SaleHash] S\n    GROUP BY\n        S.CustomerId\n) T\n\n-- COmplex Query\nSELECT\n    AVG(TotalProfit) as AvgMonthlyCustomerProfit\nFROM\n(\n    SELECT\n        S.CustomerId\n        ,D.Year\n        ,D.Month\n        ,SUM(S.TotalAmount) as TotalAmount\n        ,AVG(S.TotalAmount) as AvgAmount\n        ,SUM(S.ProfitAmount) as TotalProfit\n        ,AVG(S.ProfitAmount) as AvgProfit\n    FROM\n        [wwi_perf].[SaleHash] S\n        join [wwi].[Date] D on\n            D.DateId = S.TransactionDate\n    GROUP BY\n        S.CustomerId\n        ,D.Year\n        ,D.Month\n) T\n\n-- Improve further the structure of the table with partitioning\n-- Date columns are usually good candidates for partitioning tables at the distributions level. In the case of your sales data,\n--  partitioning based on the TransactionDateId column seems to be a good choice.\nCREATE TABLE [wwi_perf].[Sale_Partition01]\nWITH\n(\n\tDISTRIBUTION = HASH ( [CustomerId] ),\n\tCLUSTERED COLUMNSTORE INDEX,\n\tPARTITION\n\t(\n\t\t[TransactionDate] RANGE RIGHT FOR VALUES (\n            20190101, 20190201, 20190301, 20190401, 20190501, 20190601, 20190701, 20190801, 20190901, 20191001, 20191101, 20191201)\n\t)\n)\nAS\nSELECT\n\t*\nFROM\t\n\t[wwi_perf].[SaleHash]\nOPTION  (LABEL  = 'CTAS : Sale_Partition01')\n\nCREATE TABLE [wwi_perf].[Sale_Partition02]\nWITH\n(\n\tDISTRIBUTION = HASH ( [CustomerId] ),\n\tCLUSTERED COLUMNSTORE INDEX,\n\tPARTITION\n\t(\n\t\t[TransactionDate] RANGE RIGHT FOR VALUES (\n            20190101, 20190401, 20190701, 20191001)\n\t)\n)\nAS\nSELECT *\nFROM\n    [wwi_perf].[SaleHash]\nOPTION  (LABEL  = 'CTAS : Sale_Partition02')\n\n-- Notice the two partitioning strategies we've used here. The first partitioning scheme is month-based and the second is quarter-based.\n--  You will explore in the subtle differences between these and understand the potential performance implications resulting from these choices.\n\n",
			"metadata": {
				"language": "sql"
			},
			"currentConnection": {
				"databaseName": "dataaidw",
				"poolName": "dataaidw"
			},
			"resultLimit": 5000
		},
		"type": "SqlQuery"
	}
}