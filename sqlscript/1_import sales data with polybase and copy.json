{
	"name": "1_import sales data with polybase and copy",
	"properties": {
		"folder": {
			"name": "WideWorldImport/01_sales data"
		},
		"content": {
			"query": "-- There are different options for loading large amounts and varying types of data into \n-- Azure Synapse Analytics, such as through T-SQL commands using a Synapse SQL Pool, and with \n-- Azure Synapse pipelines. In our scenario, Wide World Importers stores most of their raw data in a\n--  data lake and in different formats. Among the data loading options available to them, WWI's \n-- data engineers are most comfortable using T-SQL.\n\n-- However, even with their familiarity with SQL, there are some things to consider when loading large \n-- or disparate file types and formats. Since the files are stored in ADLS Gen2, \n-- WWI can use either PolyBase external tables or the new COPY statement. \n-- Both options enable fast and scalable data load operations, but there are some differences\n--  between the two:\n\n-- PolyBase\t| COPY\n-- GA, stable |\tGA, stable\n-- Needs CONTROL permission |\tRelaxed permission\n-- Has row width limits\t| No row width limit\n-- No delimiters within text | Supports delimiters in text\n-- Fixed line delimiter |\tSupports custom column and row delimiters\n-- Complex to set up in code | Reduces amount of code\n\n-- WWI has heard that PolyBase is generally faster than COPY, especially when working with large data sets.\n\n-- The Sale table has a columnstore index to optimize for read-heavy workloads. \n-- It is also used heavily for reporting and ad-hoc queries. To achieve the fastest loading speed and \n-- minimize the impact of heavy data inserts on the Sale table, \n-- WWI has decided to create a staging table for loads.\n\n-- Create a new staging table named SaleHeap in a new schema named wwi_staging. \n-- You will define it as a heap and use round-robin distribution. When WWI finalizes their data loading \n-- pipeline, they will load the data into SaleHeap, then insert from the heap table into Sale. \n-- Although this is a two-step process, the second step of inserting the rows to the production table\n--  does not incur data movement across the distributions.\n\nCREATE SCHEMA [wwi_staging]\n\nCREATE TABLE [wwi_staging].[SaleHeap]\n( \n    [TransactionId] [uniqueidentifier]  NOT NULL,\n    [CustomerId] [int]  NOT NULL,\n    [ProductId] [smallint]  NOT NULL,\n    [Quantity] [smallint]  NOT NULL,\n    [Price] [decimal](9,2)  NOT NULL,\n    [TotalAmount] [decimal](9,2)  NOT NULL,\n    [TransactionDate] [int]  NOT NULL,\n    [ProfitAmount] [decimal](9,2)  NOT NULL,\n    [Hour] [tinyint]  NOT NULL,\n    [Minute] [tinyint]  NOT NULL,\n    [StoreId] [smallint]  NOT NULL\n)\nWITH (\n    DISTRIBUTION = ROUND_ROBIN,\n    HEAP\n)\n\nCREATE TABLE [wwi_staging].[Sale]\n(\n    [TransactionId] [uniqueidentifier]  NOT NULL,\n    [CustomerId] [int]  NOT NULL,\n    [ProductId] [smallint]  NOT NULL,\n    [Quantity] [smallint]  NOT NULL,\n    [Price] [decimal](9,2)  NOT NULL,\n    [TotalAmount] [decimal](9,2)  NOT NULL,\n    [TransactionDate] [int]  NOT NULL,\n    [ProfitAmount] [decimal](9,2)  NOT NULL,\n    [Hour] [tinyint]  NOT NULL,\n    [Minute] [tinyint]  NOT NULL,\n    [StoreId] [smallint]  NOT NULL\n)\nWITH\n(\n    DISTRIBUTION = HASH ( [CustomerId] ),\n    CLUSTERED COLUMNSTORE INDEX,\n    PARTITION\n    (\n        [TransactionDate] RANGE RIGHT FOR VALUES (20100101, 20100201, 20100301, 20100401, 20100501, 20100601, 20100701, 20100801, 20100901, 20101001, 20101101, 20101201, 20110101, 20110201, 20110301, 20110401, 20110501, 20110601, 20110701, 20110801, 20110901, 20111001, 20111101, 20111201, 20120101, 20120201, 20120301, 20120401, 20120501, 20120601, 20120701, 20120801, 20120901, 20121001, 20121101, 20121201, 20130101, 20130201, 20130301, 20130401, 20130501, 20130601, 20130701, 20130801, 20130901, 20131001, 20131101, 20131201, 20140101, 20140201, 20140301, 20140401, 20140501, 20140601, 20140701, 20140801, 20140901, 20141001, 20141101, 20141201, 20150101, 20150201, 20150301, 20150401, 20150501, 20150601, 20150701, 20150801, 20150901, 20151001, 20151101, 20151201, 20160101, 20160201, 20160301, 20160401, 20160501, 20160601, 20160701, 20160801, 20160901, 20161001, 20161101, 20161201, 20170101, 20170201, 20170301, 20170401, 20170501, 20170601, 20170701, 20170801, 20170901, 20171001, 20171101, 20171201, 20180101, 20180201, 20180301, 20180401, 20180501, 20180601, 20180701, 20180801, 20180901, 20181001, 20181101, 20181201, 20190101, 20190201, 20190301, 20190401, 20190501, 20190601, 20190701, 20190801, 20190901, 20191001, 20191101, 20191201)\n    )\n)\n\n-- Configure and run Polybase load operation\n-- PolyBase requires the following elements:\n\n-- An external data source that points to the abfss path in ADLS Gen2 where the Parquet files are located\n-- An external file format for Parquet files\n-- An external table that defines the schema for the files, as well as the location, data source, and \n-- file format\n\n-- Replace SUFFIX with the lab workspace id.\nCREATE EXTERNAL DATA SOURCE ABSS\nWITH\n( TYPE = HADOOP,\n    LOCATION = 'abfss://wwi-02@dataairawdls.dfs.core.windows.net'\n);\n\nCREATE EXTERNAL FILE FORMAT [ParquetFormat]\nWITH (\n    FORMAT_TYPE = PARQUET,\n    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n)\nGO\n\n-- Notice that we defined TransactionId as an nvarchar(36) field instead of uniqueidentifier. \n-- This is because external tables do not currently support uniqueidentifier columns:\n\nCREATE SCHEMA [wwi_external];\nGO\n\nCREATE EXTERNAL TABLE [wwi_external].Sales\n    (\n        [TransactionId] [nvarchar](36)  NOT NULL,\n        [CustomerId] [int]  NOT NULL,\n        [ProductId] [smallint]  NOT NULL,\n        [Quantity] [smallint]  NOT NULL,\n        [Price] [decimal](9,2)  NOT NULL,\n        [TotalAmount] [decimal](9,2)  NOT NULL,\n        [TransactionDate] [int]  NOT NULL,\n        [ProfitAmount] [decimal](9,2)  NOT NULL,\n        [Hour] [tinyint]  NOT NULL,\n        [Minute] [tinyint]  NOT NULL,\n        [StoreId] [smallint]  NOT NULL\n    )\nWITH\n    (\n        LOCATION = '/sale-small/Year=2019',  \n        DATA_SOURCE = ABSS,\n        FILE_FORMAT = [ParquetFormat]  \n    )  \nGO\n\n-- On DW100c, following query takes ~20 minutes\nINSERT INTO [wwi_staging].[SaleHeap]\nSELECT *\nFROM [wwi_external].[Sales]\n\n-- # of rows 339507246\nSELECT COUNT(1) FROM wwi_staging.SaleHeap(nolock)\n\n-- Configure and run the COPY statement\n-- let's see how to perform the same load operation with the COPY statement.\nTRUNCATE TABLE wwi_staging.SaleHeap;\nGO\n\n-- COPY INTO took about ~19 minute\nCOPY INTO wwi_staging.SaleHeap\nFROM 'https://dataairawdls.dfs.core.windows.net/wwi-02/sale-small%2FYear%3D2019'\nWITH (\n    FILE_TYPE = 'PARQUET',\n    COMPRESSION = 'SNAPPY'\n)\nGO\n\n-- # of rows 339507246\nSELECT COUNT(1) FROM wwi_staging.SaleHeap(nolock)\n\n--Do the number of rows match for both load operations? Which activity was fastest? \n-- You should see that both copied the same amount of data in roughly the same amount of time.\n\n-- Load data into the clustered columnstore table\n-- For both of the load operations above, we inserted data into the heap table. \n-- What if we inserted into the clustered columnstore table instead? Is there really a performance difference?\n\n-- Copy into Sales - Took ~20:21 minutes\nCOPY INTO wwi_staging.Sale\nFROM 'https://dataairawdls.dfs.core.windows.net/wwi-02/sale-small%2FYear%3D2019'\nWITH (\n    FILE_TYPE = 'PARQUET',\n    COMPRESSION = 'SNAPPY'\n)\nGO\n\nSELECT COUNT(1) FROM wwi_staging.Sale(nolock)\n-- PolyBase vs. COPY (DW100) (insert 2019 small data set (339,507,246 rows)):\n-- COPY (Heap: 18:91, clustered columnstore: 20:21)\n-- PolyBase (Heap: 19:XX)\n\n-- Use COPY to load text file with non-standard row delimiters\n-- One of the advantages COPY has over PolyBase is that it supports custom column and row delimiters.\n-- WWI has a nightly process that ingests regional sales data from a partner analytics system and \n-- saves the files in the data lake. The text files use non-standard column and row delimiters \n-- where columns are delimited by a . and rows by a ,:\n\nCREATE TABLE [wwi_staging].DailySalesCounts\n    (\n        [Date] [int]  NOT NULL,\n        [NorthAmerica] [int]  NOT NULL,\n        [SouthAmerica] [int]  NOT NULL,\n        [Europe] [int]  NOT NULL,\n        [Africa] [int]  NOT NULL,\n        [Asia] [int]  NOT NULL\n    )\nGO\n\n-- Replace <PrimaryStorage> with the workspace default storage account name.\nCOPY INTO wwi_staging.DailySalesCounts\nFROM 'https://dataairawdls.dfs.core.windows.net/wwi-02/campaign-analytics/dailycounts.txt'\nWITH (\n    FILE_TYPE = 'CSV',\n    FIELDTERMINATOR='.',\n    ROWTERMINATOR=','\n)\nGO\n\nSELECT * FROM [wwi_staging].DailySalesCounts\nORDER BY [Date] DESC\n\n-- Use PolyBase to load text file with non-standard row delimiters\nCREATE EXTERNAL FILE FORMAT csv_dailysales\nWITH (\n    FORMAT_TYPE = DELIMITEDTEXT,\n    FORMAT_OPTIONS (\n        FIELD_TERMINATOR = '.',\n        DATE_FORMAT = '',\n        USE_TYPE_DEFAULT = False\n    )\n);\nGO\n\nCREATE EXTERNAL TABLE [wwi_external].DailySalesCounts\n    (\n        [Date] [int]  NOT NULL,\n        [NorthAmerica] [int]  NOT NULL,\n        [SouthAmerica] [int]  NOT NULL,\n        [Europe] [int]  NOT NULL,\n        [Africa] [int]  NOT NULL,\n        [Asia] [int]  NOT NULL\n    )\nWITH\n    (\n        LOCATION = '/campaign-analytics/dailycounts.txt',  \n        DATA_SOURCE = ABSS,\n        FILE_FORMAT = csv_dailysales\n    )  \nGO\nINSERT INTO [wwi_staging].[DailySalesCounts]\nSELECT *\nFROM [wwi_external].[DailySalesCounts]\n\n-- You should see an error similar to: Failed to execute query. Error: HdfsBridge::recordReaderFillBuffer\n--  - Unexpected error encountered filling record reader buffer: HadoopExecutionException: Too many columns in the line..\n-- The row delimiter in delimited-text files must be supported by Hadoop's LineRecordReader. \n-- That is, it must be either \\r, \\n, or \\r\\n. These delimiters are not user-configurable.\n-- This is an example of where COPY's flexibility gives it an advantage over PolyBase.\n\n\n\n",
			"metadata": {
				"language": "sql"
			},
			"currentConnection": {
				"databaseName": "dataaidw",
				"poolName": "dataaidw"
			},
			"resultLimit": 5000
		},
		"type": "SqlQuery"
	}
}