{
	"name": "2_analyze performance",
	"properties": {
		"folder": {
			"name": "WideWorldImport/03_datawarehouse optimization"
		},
		"content": {
			"query": "--Analyze the space used by tables\nDBCC PDW_SHOWSPACEUSED('wwi_perf.SaleHash');\n\n-- Analyze the number of rows in each distribution. Those numbers should be as even as possible. \n-- You can see from the results that rows are equally distributed across distributions. Let's dive a bit more into this analysis. \n-- Use the following query to get customers with the most sale transaction items:\n\nSELECT TOP 1000\n    CustomerId,\n    count(*) as TransactionItemsCount\nFROM\n    [wwi_perf].[SaleHash]\nGROUP BY\n    CustomerId\nORDER BY\n    count(*) DESC\n\n--Now find the customers with the least sale transaction items:\nSELECT TOP 1000\n    CustomerId,\n    count(*) as TransactionItemsCount\nFROM\n    [wwi_perf].[SaleHash]\nGROUP BY\n    CustomerId\nORDER BY\n    count(*) ASC\n\n-- Notice the largest number of transaction items is 69 and the smallest is 16.\nSELECT\n    T.TransactionItemsCountBucket\n    ,count(*) as CustomersCount\nFROM\n    (\n        SELECT\n            CustomerId,\n            (count(*) - 16) / 100 as TransactionItemsCountBucket\n        FROM\n            [wwi_perf].[SaleHash]\n        GROUP BY\n            CustomerId\n    ) T\nGROUP BY\n    T.TransactionItemsCountBucket\nORDER BY\n    T.TransactionItemsCountBucket\n\n-- Use a more advanced approach to understand table space usage\n-- drop view [wwi_perf].[vTableSizes] \nCREATE VIEW [wwi_perf].[vTableSizes]\nAS\nWITH base\nAS\n(\nSELECT\n    GETDATE()                                                              AS  [execution_time]\n    , DB_NAME()                                                            AS  [database_name]\n    , s.name                                                               AS  [schema_name]\n    , t.name                                                               AS  [table_name]\n    , QUOTENAME(s.name)+'.'+QUOTENAME(t.name)                              AS  [two_part_name]\n    , nt.[name]                                                            AS  [node_table_name]\n    , ROW_NUMBER() OVER(PARTITION BY nt.[name] ORDER BY (SELECT NULL))     AS  [node_table_name_seq]\n    , tp.[distribution_policy_desc]                                        AS  [distribution_policy_name]\n    , c.[name]                                                             AS  [distribution_column]\n    , nt.[distribution_id]                                                 AS  [distribution_id]\n    , i.[type]                                                             AS  [index_type]\n    , i.[type_desc]                                                        AS  [index_type_desc]\n    , nt.[pdw_node_id]                                                     AS  [pdw_node_id]\n    , pn.[type]                                                            AS  [pdw_node_type]\n    , pn.[name]                                                            AS  [pdw_node_name]\n    , di.name                                                              AS  [dist_name]\n    , di.position                                                          AS  [dist_position]\n    , nps.[partition_number]                                               AS  [partition_nmbr]\n    , nps.[reserved_page_count]                                            AS  [reserved_space_page_count]\n    , nps.[reserved_page_count] - nps.[used_page_count]                    AS  [unused_space_page_count]\n    , nps.[in_row_data_page_count]\n        + nps.[row_overflow_used_page_count]\n        + nps.[lob_used_page_count]                                        AS  [data_space_page_count]\n    , nps.[reserved_page_count]\n    - (nps.[reserved_page_count] - nps.[used_page_count])\n    - ([in_row_data_page_count]\n            + [row_overflow_used_page_count]+[lob_used_page_count])        AS  [index_space_page_count]\n    , nps.[row_count]                                                      AS  [row_count]\nFROM\n    sys.schemas s\nINNER JOIN sys.tables t\n    ON s.[schema_id] = t.[schema_id]\nINNER JOIN sys.indexes i\n    ON  t.[object_id] = i.[object_id]\n    AND i.[index_id] <= 1\nINNER JOIN sys.pdw_table_distribution_properties tp\n    ON t.[object_id] = tp.[object_id]\nINNER JOIN sys.pdw_table_mappings tm\n    ON t.[object_id] = tm.[object_id]\nINNER JOIN sys.pdw_nodes_tables nt\n    ON tm.[physical_name] = nt.[name]\nINNER JOIN sys.dm_pdw_nodes pn\n    ON  nt.[pdw_node_id] = pn.[pdw_node_id]\nINNER JOIN sys.pdw_distributions di\n    ON  nt.[distribution_id] = di.[distribution_id]\nINNER JOIN sys.dm_pdw_nodes_db_partition_stats nps\n    ON nt.[object_id] = nps.[object_id]\n    AND nt.[pdw_node_id] = nps.[pdw_node_id]\n    AND nt.[distribution_id] = nps.[distribution_id]\nLEFT OUTER JOIN (select * from sys.pdw_column_distribution_properties where distribution_ordinal = 1) cdp\n    ON t.[object_id] = cdp.[object_id]\nLEFT OUTER JOIN sys.columns c\n    ON cdp.[object_id] = c.[object_id]\n    AND cdp.[column_id] = c.[column_id]\nWHERE pn.[type] = 'COMPUTE'\n)\n, size\nAS\n(\nSELECT\n[execution_time]\n,  [database_name]\n,  [schema_name]\n,  [table_name]\n,  [two_part_name]\n,  [node_table_name]\n,  [node_table_name_seq]\n,  [distribution_policy_name]\n,  [distribution_column]\n,  [distribution_id]\n,  [index_type]\n,  [index_type_desc]\n,  [pdw_node_id]\n,  [pdw_node_type]\n,  [pdw_node_name]\n,  [dist_name]\n,  [dist_position]\n,  [partition_nmbr]\n,  [reserved_space_page_count]\n,  [unused_space_page_count]\n,  [data_space_page_count]\n,  [index_space_page_count]\n,  [row_count]\n,  ([reserved_space_page_count] * 8.0)                                 AS [reserved_space_KB]\n,  ([reserved_space_page_count] * 8.0)/1000                            AS [reserved_space_MB]\n,  ([reserved_space_page_count] * 8.0)/1000000                         AS [reserved_space_GB]\n,  ([reserved_space_page_count] * 8.0)/1000000000                      AS [reserved_space_TB]\n,  ([unused_space_page_count]   * 8.0)                                 AS [unused_space_KB]\n,  ([unused_space_page_count]   * 8.0)/1000                            AS [unused_space_MB]\n,  ([unused_space_page_count]   * 8.0)/1000000                         AS [unused_space_GB]\n,  ([unused_space_page_count]   * 8.0)/1000000000                      AS [unused_space_TB]\n,  ([data_space_page_count]     * 8.0)                                 AS [data_space_KB]\n,  ([data_space_page_count]     * 8.0)/1000                            AS [data_space_MB]\n,  ([data_space_page_count]     * 8.0)/1000000                         AS [data_space_GB]\n,  ([data_space_page_count]     * 8.0)/1000000000                      AS [data_space_TB]\n,  ([index_space_page_count]  * 8.0)                                   AS [index_space_KB]\n,  ([index_space_page_count]  * 8.0)/1000                              AS [index_space_MB]\n,  ([index_space_page_count]  * 8.0)/1000000                           AS [index_space_GB]\n,  ([index_space_page_count]  * 8.0)/1000000000                        AS [index_space_TB]\nFROM base\n)\nSELECT *\nFROM size\n\n-- Table Name\tDescription\n-- sys.schemas |All schemas in the database.\n-- sys.tables  | All tables in the database.\n-- sys.indexes |All indexes in the database.\n-- sys.columns|All columns in the database.\n-- sys.pdw_table_mappings|Maps each table to local tables on physical nodes and distributions.\n-- sys.pdw_nodes_tables|Contains information on each local table in each distribution.\n-- sys.pdw_table_distribution_properties|Holds distribution information for tables (the type of distribution tables have).\n-- sys.pdw_column_distribution_properties|Holds distribution information for columns. Filtered to include only columns used to distribute their parent tables (distribution_ordinal = 1).\n-- sys.pdw_distributions|Holds information about the distributions from the SQL pool.\n-- sys.dm_pdw_nodes|Holds information about the nodes from the SQL pool. Filtered to include only compute nodes (type = COMPUTE).\n-- sys.dm_pdw_nodes_db_partition_stats|Returns page and row-count information for every partition in the current database.\n\n-- Run the following script to view the details about the structure of the tables in the wwi_perf schema:\nSELECT\n    database_name\n,    schema_name\n,    table_name\n,    distribution_policy_name\n,      distribution_column\n,    index_type_desc\n,    COUNT(distinct partition_nmbr) as nbr_partitions\n,    SUM(row_count)                 as table_row_count\n,    SUM(reserved_space_GB)         as table_reserved_space_GB\n,    SUM(data_space_GB)             as table_data_space_GB\n,    SUM(index_space_GB)            as table_index_space_GB\n,    SUM(unused_space_GB)           as table_unused_space_GB\nFROM\n    [wwi_perf].[vTableSizes]\nWHERE\n    schema_name = 'wwi_perf'\nGROUP BY\n    database_name\n,    schema_name\n,    table_name\n,    distribution_policy_name\n,      distribution_column\n,    index_type_desc\nORDER BY\n    table_reserved_space_GB desc\n\n-- Notice the significant difference between the space used by CLUSTERED COLUMNSTORE and HEAP or CLUSTERED tables. \n-- This provides a clear indication on the significant advantages columnstore indexes have.\n\n\ncreate view [wwi_perf].[vColumnStoreRowGroupStats]\nas\nwith cte\nas\n(\nselect   tb.[name]                    AS [logical_table_name]\n,        rg.[row_group_id]            AS [row_group_id]\n,        rg.[state]                   AS [state]\n,        rg.[state_desc]              AS [state_desc]\n,        rg.[total_rows]              AS [total_rows]\n,        rg.[trim_reason_desc]        AS trim_reason_desc\n,        mp.[physical_name]           AS physical_name\nFROM    sys.[schemas] sm\nJOIN    sys.[tables] tb               ON  sm.[schema_id]          = tb.[schema_id]\nJOIN    sys.[pdw_table_mappings] mp   ON  tb.[object_id]          = mp.[object_id]\nJOIN    sys.[pdw_nodes_tables] nt     ON  nt.[name]               = mp.[physical_name]\nJOIN    sys.[dm_pdw_nodes_db_column_store_row_group_physical_stats] rg      ON  rg.[object_id]     = nt.[object_id]\n                                                                            AND rg.[pdw_node_id]   = nt.[pdw_node_id]\n                                        AND rg.[distribution_id]    = nt.[distribution_id]\n)\nselect *\nfrom cte;\n\n-- The state_desc column provides useful information on the state of a row group:\n-- Name|Description\n-- INVISIBLE|A rowgroup which is being compressed.\n-- OPEN|A deltastore rowgroup that is accepting new rows. It is important to remember that an open rowgroup is still in rowstore format and has not been compressed to columnstore format.\n-- CLOSED|A deltastore rowgroup that contains the maximum number of rows, and is waiting for the tuple mover process to compress it to the columnstore.\n-- COMPRESSED|A row group that is compressed with columnstore compression and stored in the columnstore.\n-- TOMBSTONE|A row group that was formerly in the deltastore and is no longer used.\n\n-- The trim_reason_desc column describes the reason that triggered the COMPRESSED rowgroup to have less than the maximum\n--  number of rows:\n--Name|Description\n--UNKNOWN_UPGRADED_FROM_PREVIOUS_VERSION|Occurred when upgrading from the previous version of SQL Server.\n--NO_TRIM|The row group was not trimmed. The row group was compressed with the maximum of 1,048,476 rows. The number of rows could be less if a subset of rows was deleted after delta rowgroup was closed.\n--BULKLOAD|The bulk-load batch size limited the number of rows. This is what you should be looking for when optimizing data loading, as it is an indicator of resource starvation during the loading process.\n--REORG|Forced compression as part of REORG command.\n--DICTIONARY_SIZE|Dictionary size grew too large to compress all of the rows together.\n--MEMORY_LIMITATION|Not enough available memory to compress all the rows together.\n--RESIDUAL_ROW_GROUP|Closed as part of last row group with rows < 1 million during index build operation.\n\nSELECT\n    *\nFROM\n    [wwi_perf].[vColumnStoreRowGroupStats]\nWHERE\n    Logical_Table_Name = 'Sale_Partition01'\n\n-- Browse through the results and get an overview of the rowgroup states. \n-- Notice the COMPRESSED and OPEN states of some of the row groups.\n\nSELECT\n    *\nFROM\n    [wwi_perf].[vColumnStoreRowGroupStats]\nWHERE\n    Logical_Table_Name = 'SaleHashOrdered'\n\n-- There is a significant difference in the rowgroup states from the previous one. \n-- This highlights one of the potential advantages of ordered CCIs.\n\n--Study the impact of materialized views\n-- Analyze the execution plan of a query\nSELECT\n    T.TransactionItemsCountBucket\n    ,count(*) as CustomersCount\nFROM\n    (\n        SELECT\n            CustomerId,\n            (count(*) - 184) / 100 as TransactionItemsCountBucket\n        FROM\n            [wwi_perf].[SaleHash]\n        GROUP BY\n            CustomerId\n    ) T\nGROUP BY\n    T.TransactionItemsCountBucket\nORDER BY\n    T.TransactionItemsCountBucket\n\n-- Improve the query by adding support to calculate the lower margin of the first per-customer transactions items count bucket:\nSELECT\n    T.TransactionItemsCountBucket\n    ,count(*) as CustomersCount\nFROM\n    (\n        SELECT\n            CustomerId,\n            (\n                COUNT(*) -\n                (\n                    SELECT\n                        MIN(TransactionItemsCount)\n                    FROM\n                    (\n                        SELECT\n                            COUNT(*) as TransactionItemsCount\n                        FROM\n                            [wwi_perf].[SaleHash]\n                        GROUP BY\n                            CustomerId\n                    ) X\n                )\n            ) / 100 as TransactionItemsCountBucket\n        FROM\n            [wwi_perf].[SaleHash]\n        GROUP BY\n            CustomerId\n    ) T\nGROUP BY\n    T.TransactionItemsCountBucket\nORDER BY\n    T.TransactionItemsCountBucket\n\n-- Improve the execution plan of the query with a materialized view\n-- Run the query with the EXPLAIN directive (note the WITH_RECOMMENDATIONS option as well):\nEXPLAIN WITH_RECOMMENDATIONS\nSELECT\n    T.TransactionItemsCountBucket\n    ,count(*) as CustomersCount\nFROM\n    (\n        SELECT\n            CustomerId,\n            (\n                COUNT(*) - \n                (\n                    SELECT \n                        MIN(TransactionItemsCount)\n                    FROM \n                    (\n                        SELECT \n                            COUNT(*) as TransactionItemsCount\n                        FROM \n                            [wwi_perf].[SaleHash] \n                        GROUP BY \n                            CustomerId \n                    ) X \n                )\n            ) / 100 as TransactionItemsCountBucket\n        FROM\n            [wwi_perf].[SaleHash]\n        GROUP BY\n            CustomerId\n    ) T\nGROUP BY\n    T.TransactionItemsCountBucket\nORDER BY\n    T.TransactionItemsCountBucket\n\n-- Analyze the resulting execution plan. Take a close look to the <materialized_view_candidates> section which \n-- suggests possible materialized views you can create to improve the performance of the query.\n\n-- Create the suggested materialized view:\nCREATE MATERIALIZED VIEW\n    mvTransactionItemsCounts\nWITH\n(\n    DISTRIBUTION = HASH([CustomerId])\n)\nAS\nSELECT\n    CustomerId\n    ,COUNT(*) AS ItemsCount\nFROM\n    [wwi_perf].[SaleHash]\nGROUP BY\n    CustomerId\n\n-- Check the execution plan again:\nEXPLAIN WITH_RECOMMENDATIONS\nSELECT\n    T.TransactionItemsCountBucket\n    ,count(*) as CustomersCount\nFROM\n    (\n        SELECT\n            CustomerId,\n            (\n                COUNT(*) - \n                (\n                    SELECT \n                        MIN(TransactionItemsCount)\n                    FROM \n                    (\n                        SELECT \n                            COUNT(*) as TransactionItemsCount\n                        FROM \n                            [wwi_perf].[SaleHash] \n                        GROUP BY \n                            CustomerId \n                    ) X \n                )\n            ) / 100 as TransactionItemsCountBucket\n        FROM\n            [wwi_perf].[SaleHash]\n        GROUP BY\n            CustomerId\n    ) T\nGROUP BY\n    T.TransactionItemsCountBucket\nORDER BY\n    T.TransactionItemsCountBucket\n\n-- The resulting execution plan indicates now the use of the mvTransactionItemsCounts (the BROADCAST_MOVE distributed \n-- SQL operation) materialized view which provides improvements to the query execution time:\n\n-- Avoid extensive logging\n-- The following operations are capable of being minimally logged:\n-- CREATE TABLE AS SELECT (CTAS)\n--INSERT..SELECT\n--CREATE INDEX\n--ALTER INDEX REBUILD\n--DROP INDEX\n--TRUNCATE TABLE\n--DROP TABLE\n--ALTER TABLE SWITCH PARTITION\n--Minimal logging with bulk load\n\n--CTAS and INSERT...SELECT are both bulk load operations. However, both are influenced by the target table definition \n-- and depend on the load scenario. The following table explains when bulk operations are fully or minimally logged:\n\n--Primary Index|Load Scenario|Logging Mode\n--Heap|Any|Minimal\n--Clustered Index|Empty target table|Minimal\n--Clustered Index|Loaded rows do not overlap with existing pages in target|Minimal\n--Clustered Index|Loaded rows overlap with existing pages in target|Full\n--Clustered Columnstore Index|Batch size >= 102,400 per partition aligned distribution|Minimal\n\n-- A Synapse Analytics SQL pool has 60 distributions. Therefore, assuming all rows are evenly distributed and \n-- landing in a single partition, your batch will need to contain 6,144,000 rows or larger to be minimally logged \n-- when writing to a Clustered Columnstore Index. If the table is partitioned and the rows being inserted span\n--  partition boundaries, then you will need 6,144,000 rows per partition boundary assuming even data distribution.\n--  Each partition in each distribution must independently exceed the 102,400 row threshold for the insert \n-- to be minimally logged into the distribution.\n\n-- Optimizing a delete operation\nSELECT\n    COUNT_BIG(*) as TransactionItemsCount\nFROM\n    [wwi_perf].[SaleHash]\nWHERE\n    CustomerId < 900000\n\n-- Implement a minimal logging approach to delete transaction items for customers with ids lower than 900000. \n-- Use the following CTAS query to isolate the transaction items that should be kept:\nCREATE TABLE [wwi_perf].[SaleHashv2]\nWITH\n(\n    DISTRIBUTION = ROUND_ROBIN,\n    HEAP\n)\nAS\nSELECT\n    *\nFROM\n    [wwi_perf].[SaleHash]\nWHERE\n    CustomerId >= 900000\n\n-- The query should execute within a few minutes. All that would remain to complete the process would be to delete the \n-- Sale_Heap table and rename Sale_Heap_v2 to Sale_Heap.\n--DELETE\n--    [wwi_perf].[SaleHash]\n--WHERE\n--    CustomerId < 900000",
			"metadata": {
				"language": "sql"
			},
			"currentConnection": {
				"databaseName": "dataaidw",
				"poolName": "dataaidw"
			},
			"resultLimit": 5000
		},
		"type": "SqlQuery"
	}
}