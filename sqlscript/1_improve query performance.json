{
	"name": "1_improve query performance",
	"properties": {
		"folder": {
			"name": "WideWorldImport/03_datawarehouse optimization"
		},
		"content": {
			"query": "\n-- Query takes up to 60 seconds to execute. That is expected, since distinct counts are one of the most difficult to optimize types of queries.\nSELECT COUNT( DISTINCT CustomerId) from wwi_staging.SaleHeap\n\n-- Run the Hyperlog approach\n-- Query takes about half the time to execute.\nSELECT APPROX_COUNT_DISTINCT(CustomerId) from wwi_staging.SaleHeap\n\n-- Use materialized views. As opposed to a standard view, a materialized view pre-computes, stores, and maintains its data in a Synapse SQL pool \n-- just like a table. Here is a basic comparison between standard and materialized views:\n\n--Comparison |\tView | Materialized View\n--View definition |\tStored in Azure data warehouse.\t| Stored in Azure data warehouse.\n--View content | Generated each time when the view is used. | Pre-processed and stored in Azure data warehouse during view creation. Updated as data is added to the underlying tables.\n--Data refresh| Always updated | Always updated\n--Speed to retrieve view data from complex queries | \tSlow | Fast\n--Extra storage| No| Yes\n--Syntax | CREATE VIEW\t| CREATE MATERIALIZED VIEW AS SELECT\n\n-- 11 Seconds\nSELECT TOP 1000 * FROM\n(\n    SELECT\n        S.CustomerId\n        ,D.Year\n        ,D.Quarter\n        ,SUM(S.TotalAmount) as TotalAmount\n    FROM\n        [wwi_perf].[Sale_Partition02] S\n        join [wwi].[Date] D on\n            S.TransactionDate = D.DateId\n    GROUP BY\n        S.CustomerId\n        ,D.Year\n        ,D.Quarter\n) T\n\n-- 1 Second\nSELECT TOP 1000 * FROM\n(\n    SELECT\n        S.CustomerId\n        ,D.Year\n        ,D.Month\n        ,SUM(S.ProfitAmount) as TotalProfit\n    FROM\n        [wwi_perf].[Sale_Partition02] S\n        join [wwi].[Date] D on\n            S.TransactionDate = D.DateId\n    GROUP BY\n        S.CustomerId\n        ,D.Year\n        ,D.Month\n) T\n\n-- Create materialized view\nCREATE MATERIALIZED VIEW\n    wwi_perf.mvCustomerSales\nWITH\n(\n    DISTRIBUTION = HASH( CustomerId )\n)\nAS\nSELECT\n    S.CustomerId\n    ,D.Year\n    ,D.Quarter\n    ,D.Month\n    ,SUM(S.TotalAmount) as TotalAmount\n    ,SUM(S.ProfitAmount) as TotalProfit\nFROM\n    [wwi_perf].[Sale_Partition02] S\n    join [wwi].[Date] D on\n        S.TransactionDate = D.DateId\nGROUP BY\n    S.CustomerId\n    ,D.Year\n    ,D.Quarter\n    ,D.Month\n\n-- Run the following query to get an estimated execution plan (do not select Query Plan from the toolbar as it will try \n-- do download the query plan and open it in SQL Server Management Studio):\nEXPLAIN\nSELECT TOP 1000 * FROM\n(\n    SELECT\n        S.CustomerId\n        ,D.Year\n        ,D.Quarter\n        ,SUM(S.TotalAmount) as TotalAmount\n    FROM\n        [wwi_perf].[Sale_Partition02] S\n        join [wwi].[Date] D on\n            S.TransactionDate = D.DateId\n    GROUP BY\n        S.CustomerId\n        ,D.Year\n        ,D.Quarter\n) T\n\n-- The same materialized view is also used to optimize the second query. Get its execution plan:\nEXPLAIN\nSELECT TOP 1000 * FROM\n(\n    SELECT\n        S.CustomerId\n        ,D.Year\n        ,D.Month\n        ,SUM(S.ProfitAmount) as TotalProfit\n    FROM\n        [wwi_perf].[Sale_Partition02] S\n        join [wwi].[Date] D on\n            S.TransactionDate = D.DateId\n    GROUP BY\n        S.CustomerId\n        ,D.Year\n        ,D.Month\n) T\n\nDBCC PDW_SHOWMATERIALIZEDVIEWOVERHEAD ( 'wwi_perf.mvCustomerSales' )\n\n-- The results show that BASE_VIEW_ROWS are equal to TOTAL_ROWS (and hence OVERHEAD_RATIO is 1). \n-- The materialized view is perfectly aligned with the base view. This situation is expected to change once the underlying data starts to change.\n\nUPDATE\n    [wwi_perf].[Sale_Partition02]\nSET\n    TotalAmount = TotalAmount * 1.01\n    ,ProfitAmount = ProfitAmount * 1.01\nWHERE\n    CustomerId BETWEEN 100 and 200\n\n-- There is now a delta stored by the materialized view which results in TOTAL_ROWS being greater \n-- than BASE_VIEW_ROWS and OVERHEAD_RATIO being greater than 1.\nDBCC PDW_SHOWMATERIALIZEDVIEWOVERHEAD ( 'wwi_perf.mvCustomerSales' )\n\n-- Rebuild the materialized view and check that the overhead ration went back to 1:\nALTER MATERIALIZED VIEW [wwi_perf].[mvCustomerSales] REBUILD\n\nDBCC PDW_SHOWMATERIALIZEDVIEWOVERHEAD ( 'wwi_perf.mvCustomerSales' )\n\n-- Use result set caching\nSELECT\n    name\n    ,is_result_set_caching_on\nFROM\n    sys.databases\n\n-- If False is returned for your SQL pool, run the following query to activate it (you need to run it on the master database and \n-- replace <sql_pool> with the name of your SQL pool):\nALTER DATABASE dataaidw\nSET RESULT_SET_CACHING ON\n\n-- The operations to create result set cache and retrieve data from the cache happen on the control node of a Synapse SQL pool instance. \n-- When result set caching is turned ON, running queries that return large result set (for example, >1GB) can cause high throttling on \n-- the control node and slow down the overall query response on the instance. Those queries are commonly used during data exploration or\n--  ETL operations. To avoid stressing the control node and cause performance issue, users should turn OFF result set caching on the \n-- database before running those types of queries.\n\n\n-- After activating result set caching, run a query and immediately check if it hit the cache (change the database back to your SQL Pool):\nSELECT\n    D.Year\n    ,D.Quarter\n    ,D.Month\n    ,SUM(S.TotalAmount) as TotalAmount\n    ,SUM(S.ProfitAmount) as TotalProfit\nFROM\n    [wwi_perf].[Sale_Partition02] S\n    join [wwi].[Date] D on\n        S.TransactionDate = D.DateId\nGROUP BY\n    D.Year\n    ,D.Quarter\n    ,D.Month\nOPTION (LABEL = 'Result set caching')\n\nSELECT\n    result_cache_hit\nFROM\n    sys.dm_pdw_exec_requests\nWHERE\n    request_id =\n    (\n        SELECT TOP 1\n            request_id\n        FROM\n            sys.dm_pdw_exec_requests\n        WHERE\n            [label] = 'Result set caching'\n        ORDER BY\n            start_time desc\n    )\n\n-- As expected, the result is False. Still, you can identify that, while running the query, Synapse has also cached the result set. \n-- Run the following query to get the execution steps:\nSELECT\n    step_index\n    ,operation_type\n    ,location_type\n    ,status\n    ,total_elapsed_time\n    ,command\nFROM\n    sys.dm_pdw_request_steps\nWHERE\n    request_id =\n    (\n        SELECT TOP 1\n            request_id\n        FROM\n            sys.dm_pdw_exec_requests\n        WHERE\n            [label] = 'Result set caching'\n        ORDER BY\n            start_time desc\n    )\n\n-- You can control at the user session level the use of the result set cache. The following query shows how to deactivate and \n-- activate the result cache:\nSET RESULT_SET_CACHING OFF\n\nSELECT\n    D.Year\n    ,D.Quarter\n    ,D.Month\n    ,SUM(S.TotalAmount) as TotalAmount\n    ,SUM(S.ProfitAmount) as TotalProfit\nFROM\n    [wwi_perf].[Sale_Partition02] S\n    join [wwi].[Date] D on\n        S.TransactionDateId = D.DateId\nGROUP BY\n    D.Year\n    ,D.Quarter\n    ,D.Month\nOPTION (LABEL = 'Result set caching off')\n\nSET RESULT_SET_CACHING ON\n\nSELECT\n    D.Year\n    ,D.Quarter\n    ,D.Month\n    ,SUM(S.TotalAmount) as TotalAmount\n    ,SUM(S.ProfitAmount) as TotalProfit\nFROM\n    [wwi_perf].[Sale_Partition02] S\n    join [wwi].[Date] D on\n        S.TransactionDate = D.DateId\nGROUP BY\n    D.Year\n    ,D.Quarter\n    ,D.Month\nOPTION (LABEL = 'Result set caching on')\n\nSELECT TOP 2\n    request_id\n    ,[label]\n    ,result_cache_hit\nFROM\n    sys.dm_pdw_exec_requests\nWHERE\n    [label] in ('Result set caching off', 'Result set caching on')\nORDER BY\n    start_time desc\n\n-- The result of SET RESULT_SET_CACHING OFF is visible in the cache hit test results (The result_cache_hit column returns 1 for cache hit, \n-- 0 for cache miss, and negative values for reasons why result set caching was not used.):\n\n-- At any moment, you can check the space used by the results cache:\nDBCC SHOWRESULTCACHESPACEUSED\n\n-- Finally, disable result set caching on the database using the following query (you need to run it on the master database and replace \n-- `<sql_pool> with the name of your SQL pool): (It can take up to a minute to disable Result Set Caching.)\nALTER DATABASE [dataaidw]\nSET RESULT_SET_CACHING OFF\n\n-- Make sure you disable result set caching on the SQL pool. Failing to do so will have a negative impact on the remainder of this lab, \n-- as it will skew execution times and defeat the purpose of several upcoming exercises.\n\n-- The maximum size of result set cache is 1 TB per database. The cached results are automatically invalidated when the underlying query data change.\n--The cache eviction is managed by SQL Analytics automatically following this schedule:\n-- Every 48 hours if the result set hasn't been used or has been invalidated.\n-- When the result set cache approaches the maximum size.\n-- Users can manually empty the entire result set cache by using one of these options:\n\n-- Turn OFF the result set cache feature for the database\n-- Run DBCC DROPRESULTSETCACHE while connected to the database\n-- Pausing a database won't empty cached result set.\n\n\n-- Create and update statistics\n-- The more the SQL pool resource knows about your data, the faster it can execute queries. After loading data into SQL pool, \n-- collecting statistics on your data is one of the most important things you can do for query optimization.\n-- The SQL pool query optimizer is a cost-based optimizer. It compares the cost of various query plans, and then chooses the plan with \n-- the lowest cost. In most cases, it chooses the plan that will execute the fastest.\n-- For example, if the optimizer estimates that the date your query is filtering on will return one row it will choose one plan.\n--  If it estimates that the selected date will return 1 million rows, it will return a different plan.\n\n-- Check if statistics are set to be automatically created in the database:\nSELECT name, is_auto_create_stats_on\nFROM sys.databases\n\n-- See statistics that have been automatically created (change the database back to your SQL Pool):\nSELECT\n    *\nFROM\n    sys.dm_pdw_exec_requests\nWHERE\n    Command like 'CREATE STATISTICS%'\n\n-- Check if there are any statistics created for CustomerId from the wwi_perf.Sale_Has table:\nDBCC SHOW_STATISTICS ('wwi_perf.SaleHash', CustomerId) WITH HISTOGRAM\n\n-- You should get an error stating that statistics for CustomerId does not exist.\n-- Create statistics for CustomerId:\nCREATE STATISTICS Sale_Hash_CustomerId ON wwi_perf.SaleHash (CustomerId)\n\n-- Display the newly created statistics:\nDBCC SHOW_STATISTICS([wwi_perf.SaleHash], 'Sale_Hash_CustomerId')\n\n-- The more SQL pool knows about your data, the faster it can execute queries against it. After loading data into SQL pool, collecting \n-- statistics on your data is one of the most important things you can do to optimize your queries.\n-- The SQL pool query optimizer is a cost-based optimizer. It compares the cost of various query plans, and then chooses the plan with the\n--  lowest cost. In most cases, it chooses the plan that will execute the fastest.\n-- For example, if the optimizer estimates that the date your query is filtering on will return one row it will choose one plan. \n-- If it estimates that the selected date will return 1 million rows, it will return a different plan.\n\n-- Create and update indexes\n-- Clustered Columnstore Index vs. Heap vs. Clustered and Nonclustered\n-- Clustered indexes may outperform clustered columnstore indexes when a single row needs to be quickly retrieved. \n-- For queries where a single or very few row lookup is required to perform with extreme speed, consider a cluster index or \n-- nonclustered secondary index. The disadvantage to using a clustered index is that only queries that benefit are the ones \n-- that use a highly selective filter on the clustered index column. To improve filter on other columns a nonclustered index can \n-- be added to other columns. However, each index which is added to a table adds both space and processing time to loads.\n\n-- Retrieve information about a single customer from the table with CCI:\nSELECT\n    *\nFROM\n    [wwi_perf].[SaleHash]\nWHERE\n    CustomerId = 500000\n\n-- Retrieve information about a single customer from the table with a clustered index:\nSELECT\n    *\nFROM\n    [wwi_perf].[Sale_Index]\nWHERE\n    CustomerId = 500000\n\n-- The execution time is similar to the one for the query above. Clustered columnstore indexes have no significant advantage over \n-- clustered indexes in the specific scenario of highly selective queries.\n\n-- Retrieve information about multiple customers from the table with CCI:\nSELECT\n    *\nFROM\n    [wwi_perf].[SaleHash]\nWHERE\n    CustomerId between 400000 and 400100\n\n-- and then retrieve the same information from the table with a clustered index:\nSELECT\n    *\nFROM\n    [wwi_perf].[Sale_Index]\nWHERE\n    CustomerId between 400000 and 400100\n\n-- Run both queries several times to get a stable execution time. Under normal conditions, you should see that even with a \n-- relatively small number of customers, the CCI table starts yielding better results than the clustered index table.\n\n-- Now add an extra condition on the query, one that refers to the StoreId column:\nSELECT\n    *\nFROM\n    [wwi_perf].[Sale_Index]\nWHERE\n    CustomerId between 400000 and 400100\n    and StoreId between 2000 and 4000\n\n-- Create a non-clustered index on the StoreId column:\nCREATE INDEX Store_Index on wwi_perf.Sale_Index (StoreId)\n\n-- Creating a non-clustered index on the wwi_perf.Sale_Index is based on the already existing clustered index. As a bonus exercise, \n-- try to create the same type of index on the wwi_perf.Sale_Hash table. Can you explain the difference in index creation time?\n\n-- Ordered Clustered Columnstore Indexes\n-- By default, for each table created without an index option, an internal component (index builder) creates a non-ordered clustered \n-- columnstore index (CCI) on it. Data in each column is compressed into a separate CCI rowgroup segment. There's metadata on each \n-- segment's value range, so segments that are outside the bounds of the query predicate aren't read from disk during query execution. \n-- CCI offers the highest level of data compression and reduces the size of segments to read so queries can run faster. However, \n-- because the index builder doesn't sort data before compressing them into segments, segments with overlapping value ranges could occur, \n-- causing queries to read more segments from disk and take longer to finish.\n\n-- When creating an ordered CCI, the Synapse SQL engine sorts the existing data in memory by the order key(s) before the index builder \n-- compresses them into index segments. With sorted data, segment overlapping is reduced allowing queries to have a more efficient \n-- segment elimination and thus faster performance because the number of segments to read from disk is smaller. \n-- If all data can be sorted in memory at once, then segment overlapping can be avoided. Due to large tables in data warehouses, \n-- this scenario doesn't happen often.\n\n-- Queries with the following patterns typically run faster with ordered CCI:\n--The queries have equality, inequality, or range predicates\n--The predicate columns and the ordered CCI columns are the same.\n--The predicate columns are used in the same order as the column ordinal of ordered CCI columns.\n\n-- Run the following query to show the segment overlaps for the Sale_Hash table:\nselect\n    OBJ.name as table_name\n    ,COL.name as column_name\n    ,NT.distribution_id\n    ,NP.partition_id\n    ,NP.rows as partition_rows\n    ,NP.data_compression_desc\n    ,NCSS.segment_id\n    ,NCSS.version\n    ,NCSS.min_data_id\n    ,NCSS.max_data_id\n    ,NCSS.row_count\nfrom\n    sys.objects OBJ\n    JOIN sys.columns as COL ON\n        OBJ.object_id = COL.object_id\n    JOIN sys.pdw_table_mappings TM ON\n        OBJ.object_id = TM.object_id\n    JOIN sys.pdw_nodes_tables as NT on\n        TM.physical_name = NT.name\n    JOIN sys.pdw_nodes_partitions NP on\n        NT.object_id = NP.object_id\n        and NT.pdw_node_id = NP.pdw_node_id\n        and substring(TM.physical_name, 40, 10) = NP.distribution_id\n    JOIN sys.pdw_nodes_column_store_segments NCSS on\n        NP.partition_id = NCSS.partition_id\n        and NP.distribution_id = NCSS.distribution_id\n        and COL.column_id = NCSS.column_id\nwhere\n    OBJ.name = 'SaleHash'\n    and COL.name = 'CustomerId'\n    and TM.physical_name  not like '%HdTable%'\norder by\n    NT.distribution_id\n\n-- Browse through the result set and notice the significant overlap between segments. There is literally overlap in customer \n-- ids between every single pair of segments (CustomerId values in the data range from 1 to 1,000,000). The segment structure of\n--  this CCI is clearly inefficient and will result in a lot of unnecessary reads from storage.\n\nCREATE TABLE [wwi_perf].[SaleHashOrdered]\nWITH\n(\n    DISTRIBUTION = HASH ( [CustomerId] ),\n    CLUSTERED COLUMNSTORE INDEX ORDER( [CustomerId] )\n)\nAS\nSELECT\n    *\nFROM\n    [wwi_staging].[SaleHeap]\nOPTION  (LABEL  = 'CTAS : Sale_Hash', MAXDOP 1)\n\n-- Run the following query to show the segment overlaps for the Sale_Hash_Ordered table:\nselect\n    OBJ.name as table_name\n    ,COL.name as column_name\n    ,NT.distribution_id\n    ,NP.partition_id\n    ,NP.rows as partition_rows\n    ,NP.data_compression_desc\n    ,NCSS.segment_id\n    ,NCSS.version\n    ,NCSS.min_data_id\n    ,NCSS.max_data_id\n    ,NCSS.row_count\nfrom\n    sys.objects OBJ\n    JOIN sys.columns as COL ON\n        OBJ.object_id = COL.object_id\n    JOIN sys.pdw_table_mappings TM ON\n        OBJ.object_id = TM.object_id\n    JOIN sys.pdw_nodes_tables as NT on\n        TM.physical_name = NT.name\n    JOIN sys.pdw_nodes_partitions NP on\n        NT.object_id = NP.object_id\n        and NT.pdw_node_id = NP.pdw_node_id\n        and substring(TM.physical_name, 40, 10) = NP.distribution_id\n    JOIN sys.pdw_nodes_column_store_segments NCSS on\n        NP.partition_id = NCSS.partition_id\n        and NP.distribution_id = NCSS.distribution_id\n        and COL.column_id = NCSS.column_id\nwhere\n    OBJ.name = 'SaleHashOrdered'\n    and COL.name = 'CustomerId'\n    and TM.physical_name  not like '%HdTable%'\norder by\n    NT.distribution_id\n\n-- Notice the creation of the ordered CCI with MAXDOP = 1. Each thread used for ordered CCI creation works on a subset of data and \n-- sorts it locally. There's no global sorting across data sorted by different threads. Using parallel threads can reduce the time to \n-- create an ordered CCI but will generate more overlapping segments than using a single thread. Currently, the MAXDOP option is only \n-- supported in creating an ordered CCI table using CREATE TABLE AS SELECT command. Creating an ordered CCI via CREATE INDEX or CREATE \n-- TABLE commands does not support the MAXDOP option.\n\n\n",
			"metadata": {
				"language": "sql"
			},
			"currentConnection": {
				"databaseName": "dataaidw",
				"poolName": "dataaidw"
			},
			"resultLimit": 5000
		},
		"type": "SqlQuery"
	}
}